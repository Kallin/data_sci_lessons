{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Intro to Pandas\n",
    "\n",
    "<img src=\"https://wow.gamepedia.com/media/wow.gamepedia.com/6/6c/Twopandaren.jpg\" alt=\"Drawing\" style=\"width: 300px; float: right; padding: 40px\"/>\n",
    "\n",
    "What is Pandas?\n",
    "\n",
    "From the homepage...\n",
    "> pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "And from the docs...\n",
    "> pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way toward this goal.\n",
    "\n",
    "It's worth at least reading through the overview of features [here](http://pandas-docs.github.io/pandas-docs-travis/), and trying out their '10 minutes to Pandas' feature [here](http://pandas-docs.github.io/pandas-docs-travis/10min.html). There is also a great video series available [here](https://www.safaribooksonline.com/library/view/introduction-to-pandas/9781771375764/) for more details.\n",
    "\n",
    "For our purposes, pandas is a library that makes working with large arrays of data easy, much like a spreadsheet tool. We can import excel files, CSVs, etc., and look at the files from the perspective of rows and columns. If we only used NumPy, we would be looking at nameless arrays and it would be more difficult to wrap our mind around the manipulations we are performing on the data.\n",
    "\n",
    "A huge part of machine learning (some would say the biggest), is the data-cleaning and feature-engineering phase. This is where we take our raw input data and manipulate it into something that will act as good training data for ML algorithms. Pandas is extremely useful for these two tasks.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset Example\n",
    "<img src=\"http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2015/04/iris_petal_sepal.png\" alt=\"Drawing\" style=\"width: 200px; float: right; padding: 10px\"/>\n",
    "\n",
    "One of the most famous datasets in ML is the 'Iris Dataset'. Created in the 1930s to help research some pre-computer classification techniques, it samples 150 flowers from 3 separate species, each with 4 measurements, along with the actual species of that flower.\n",
    "\n",
    "The ML goal when looking at this set is to develop a model that can predict what species of flower a sample is when presented with those 4 measurements.\n",
    "\n",
    "Let's look at the iris dataset, and see how we can use Pandas to do some basic manipulation and analysis on it.\n",
    "\n",
    "Execute the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually we would load data from a CSV or other data file, but Scikit comes with some popular datasets built-in.\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what is in the iris object Scikit gave us:\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the features?\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the targets?\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the target data look like?\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the data look like? (let's just look at first 5 rows)\n",
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be a bit cumbersome dealing with raw arrays, and numbers instead of labels. Pandas gives us a much nicer API to explore the data with, but first we need to build the core Pandas object, the **DataFrame**.\n",
    "\n",
    "DataFrames can be built in many different ways, including loading CSVs, Excel files, connecting to Databases, etc. Here, we're going to build one using the Arrays coming from the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's this easy!\n",
    "# note how it has auto-assigned an index (far-left column).\n",
    "# these indices become a powerful feature of Pandas as dig into it.\n",
    "\n",
    "iris_features_df = pd.DataFrame(iris.data)\n",
    "iris_features_df.columns = iris.feature_names\n",
    "iris_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create another DataFrame to hold the labels, or target data from the data.\n",
    "# use the head method to just see the first few rows\n",
    "iris_targets_df = pd.DataFrame(iris.target)\n",
    "iris_targets_df.columns = ['label']\n",
    "iris_targets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames can be combined using joins, much like in SQL.\n",
    "# Let's join the feature and target DFs based on their index values:\n",
    "\n",
    "iris_df = pd.merge(left_index=True, right_index=True, left=iris_features_df, right=iris_targets_df)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's starting to look good, but it's tough to remember what 0,1, and 2 represent in the label column.\n",
    "# we can easily map those values to more meaningful terms.\n",
    "label_map = {\n",
    "    0: 'setosa',\n",
    "    1: 'versicolor',\n",
    "    2: 'virginica'\n",
    "}\n",
    "iris_df = iris_df.replace({'label': label_map})\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas knows the types of the different data stored inside it\n",
    "iris_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas can give us quick summaries of the data contained within a DF\n",
    "iris_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-numeric, or 'categorical' values, we can see how they break down by frequency\n",
    "iris_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we want to dig in on 'versicolor' ? \n",
    "# we can create a 'mask', which is a boolean array indicating whether something is true or not for a given row.\n",
    "versicolor_mask = iris_df['label'] == 'versicolor'\n",
    "versicolor_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then use that mask against the dataframe to view the filtered data\n",
    "iris_df[versicolor_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then perform analysis on that filtered data\n",
    "iris_df[versicolor_mask].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean with for versicolor's was 2.77cm, but overall it was 3.05\n",
    "# maybe we can just find versicolors based on this? \n",
    "small_width_mask = iris_df['sepal width (cm)'] < 2.5\n",
    "iris_df[small_width_mask]['label'].value_counts()\n",
    "\n",
    "# try changing the treshhold and see how it manages to find them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use more complex boolean queries\n",
    "\n",
    "complex_mask = (iris_df['sepal width (cm)'] < 3) \\\n",
    "    & (iris_df['sepal length (cm)'] < 6 )\n",
    "    \n",
    "maybe_versicolor_df = iris_df[complex_mask]\n",
    "maybe_versicolor_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Decision Tree](http://scikit-learn.org/stable/modules/tree.html) algorithm would automatically build a more sophisticated version of the rule above to classify versicolor, and all of the other labels, and it does a pretty good job!\n",
    "\n",
    "Pandas also makes it easy to output your DataFrames to CSVs, for analysis in Excel/etc. or for sharing with colleagues. Try running the following cell and see if you can open the resultant spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to CSV for further analysis\n",
    "\n",
    "import os\n",
    "os.makedirs('./tmp', exist_ok=True)\n",
    "maybe_versicolor_df.to_csv('./tmp/maybe_versi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "This is not even skimming the surface of Pandas' capabilties. Some other important features include handling of missing data, tools for time-series data, and some sophisticated grouping features for things like aggregation and filtering. It's worth browsing the [Pandas Docs](https://pandas.pydata.org/pandas-docs/stable/#) to get a sense of all that is possible.\n",
    "\n",
    "It's also worth noting that while this dataset only had 150 rows, Pandas is focused on speed, and can handle datasets with millions of rows very quickly. If you ever have trouble with a spreadsheet in Excel, it might be worth trying to load it in Pandas and do your queries there to save some time!\n",
    "\n",
    "#### Congratulations !\n",
    "\n",
    "You are now familiar with Jupyter, Python, Numpy, and Pandas. You're ready to start digging into some real ML tools now. \n",
    "\n",
    "To be continued..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
