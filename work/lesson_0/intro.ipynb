{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What is Machine Learning (and why should you care?)\n",
    "\n",
    "---\n",
    "![comic](https://eugene-kaspersky-wpengine.netdna-ssl.com/files/2016/09/machine-learning-robots-dilbert.gif)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What EXACTLY is Machine Learning?**\n",
    "\n",
    "> *ML*: Gives computers ability to learn without being told explicitly what to do. Often they improve with experience. \n",
    "\n",
    "**What about deep-learning/AI.. Is that the same thing?**\n",
    "\n",
    "> *Deep Learning*: A software technique that imitates the workings of the human brain to process data and detect patterns for use in making decisions. A sub-category of machine learning.\n",
    "\n",
    "It turns out there are many other sub-categories of ML besides deep-learning, and they all have their own strengths/weaknesses, and areas of application. We'll try to cover as many as we can!\n",
    "\n",
    "Ultimately, ML is in the business of creating **MODELS**. These Models are custom built for each specific task, often by feeding them with lots of **TRAINING** data. Once they have been built, they then offer **PREDICTIONS**, which can take a number of forms. Input is turned into Output, just like any other computer program.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's with all the hype (and why now?)\n",
    "\n",
    "ML has been around for decades, so why is it gaining so much traction all of a sudden? It seems every company is doing something with AI. It comes down to a confluence of a few things:\n",
    "\n",
    "**Developer Friendliness**\n",
    "\n",
    "ML, for the longest time, used to be the purvue of PHDs, universities, and perhaps a few larger companies who could afford to make big R&D investments. The algorithms are complex and the related math is more sophisticated than what most developers are accustomed to.\n",
    "\n",
    "Within the past few years open source libraries have been developed that abstract away a lot of the complexity of the underyling algorithms, and instead let developers focus on tuning the best possible model for their particular problem. These libraries offer tools to help developers verify and diagnose the performance of their models, and give us comprehendable data about how well or poorly the models will perform against unseen data.\n",
    "\n",
    "For example, look at the wikipedia page for a very popular ML algorithm like [K-Nearest-Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). You no longer *need* to understand the intricacies of this and other similar algorithms, and can instead focus on finding the right one for the job.\n",
    "\n",
    "**Performance**\n",
    "\n",
    "With the explosion of cloud services like AWS, any developer has access to nearly unlimited computing power on demand. Some new and novel parallelization libraries like Google's [Tensor Flow](https://www.tensorflow.org/) which is used for training Neural Network Models, can be used to train a model in a massively parallel fashion on specialized hardware such as a GPU (Graphics Processing Unit, yay video games!).  For instance, look at the 'Accelerated Computing: P2' option available at [AWS](https://aws.amazon.com/ec2/instance-types/). Anyone can provision one of these, with 192 GB of video memory, and 16 GPUs each with 2496 cores (~40,000 cores). They can access this machine for roughly $7/hr, and prices continue to drop as performance improves. Compare that to running on your local machine which has maybe 8 or 16 cores.\n",
    "\n",
    "This type of hardware allows training of models of a size and at a speed that simply wasn't possible until very recently.\n",
    "\n",
    "\n",
    "**Very Big Data**\n",
    "\n",
    "Companies like Google, Amazon, Facebook, etc, have so much data that they can train models that would be impossible for those on the outside to immitate. **Training Data** is a key ingredient, along with the Performance and Developer Friendliness described above to build interesting models, and they have more of it than just about anyone. It has allowed them to make remarkable advances in problems like image recognition, where now their models [can outperform humans](https://www.extremetech.com/extreme/233746-ai-beats-doctors-at-visual-diagnosis-observes-many-times-more-lung-cancer-signals) in classifying the objects in images. \n",
    "\n",
    "Thankfully for everyone else, there is a trickle down effect, and techniques are available to import the results of their training (a piece of a neural network for example), and then customize it to your specific, smaller dataset. There is also an explosion of publicly available data, and private companies (perhaps like yours!) are all collecting more data than ever that they can use to train models and automate more and more decision making.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use Machine Learning?\n",
    "\n",
    "While the applications of ML seem limitless, there are a few specific types of problems for which it has been especially useful:\n",
    "\n",
    "- complex rules, unrealistic to program manually. eg. spam/fraud detection.\n",
    "- problems without known algorithmic solutions. speech/image recognition, language translation etc.\n",
    "- coping with changing data. rules can automatically adjust on the fly without reprogramming.\n",
    "- helping humans learn. ML can find patterns in data humans never would.\n",
    "- predictions: eg. shopping predictions, chatbots, weather/stock predictions.\n",
    "\n",
    "Some specific examples:\n",
    "\n",
    "- [Kaggle](https://www.kaggle.com/competitions?sortBy=deadline&group=all&page=1)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's that easy!\n",
    "\n",
    "Let's show how we can train a well performing model and make predictions using MNIST, it is a well known set of hand-drawn digits (70,000), that has been labeled. Our goal is to build a model that can look at a hand drawn digit and recognize it.\n",
    "\n",
    "*(In order to run the code below, hit ctrl-enter while each cell is highlited. We'll go over running code in Jupyter in more detail in the next chapter.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplotlib\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch the data\n",
    "# X is the data, y are the labels\n",
    "mnist = fetch_mldata('MNIST original', data_home='./tmp')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# 'train' data is used to build the model. 'test' data is used to validate that it's working.\n",
    "# we won't show the model the test data, so it can't cheat!\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# we shuffle the training data so that it is not organized in any particular way which might affect training.\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, we have 60,000 training instances, and 10,000 we will test against.\n",
    "\n",
    "# let's pick a test digit at random\n",
    "index = np.random.randint(10000)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see what that digit looks like. It's a 28x28 array of pixel intensities, 0 to 255.\n",
    "image = np.reshape(X_test[index], (28,28))\n",
    "pd.DataFrame(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what this number actually looks like\n",
    "plt.imshow(image, cmap = matplotlib.cm.binary,interpolation=\"nearest\")\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's see what it was supposed to be\n",
    "y_test[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we know what this digit looks like and what it's supposed to be. \n",
    "Can we use ML to build a model that can accurately recognize these digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  we'll use K-Nearest Neighbors as it works quite well. \n",
    "# 'weights=distance' and 'n_neighbors=4' are called 'hyperparameters'. Hyperparameters let us \n",
    "# tune an algorithm to work better with our specific data. How we choose these values is explained later.\n",
    "best_knn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4, n_jobs=-1)\n",
    "\n",
    "best_knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_knn_clf.predict_proba(X_test[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your model predict your image's value correctly? It probably did, even though it never saw that particular drawing before. It can shown that this particular model successfully recognizes about 97% of new images. With a little more time and tuning, we can get closer to 100%.\n",
    "\n",
    "Let's head back to the [Table Of Contents](../table_of_contents.ipynb) where we can take a closer look at some of the tools we'll be making use of.\n",
    "\n",
    "Oh, and maybe run the next cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![comic](https://raw.githubusercontent.com/qingkaikong/blog/master/2017_12_machine_learning_funny_pictures/figures/figure_8.png)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
